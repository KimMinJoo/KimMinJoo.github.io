---
layout: post
title: 기계학습스터디] Naive Bayes
---

### 확률 이론으로 분류하기

### 베이즈 정리(Bayes' theorem)
확률론과 통계학에서, 베이즈 정리는 두 확률변수의 사전 확률과 사후 확률사이의 관계를 나타내는 정리이다.<br>
식으로 표현할 경우 아래와 같다.<br>
<br>
<img src="https://github.com/KimMinJoo/KimMinJoo.github.io/blob/master/images/BayesTheorem.jpg?raw=true"/>

식에 대해 알아보기전에 용어 정리를 해보자.<br>

- 사전확률 : 이미 알고있는 확률로 위의 식에서 P(A) 들을 의미한다.
- 우도 : 조건부확률의 일종으로 이미 사전확률을 알고 있는 사건들이 발생했을때 다른 사건이 발생하는것을 의미한다.
- 사후확률 : 사전확률과 우도를 통해서 알게되는 조건부 확률로 P(A|B)를 의미한다.

이제 식에대해 알아보자.<br>
기본 P(A|B)에서 식.1로 넘어가는것은 조건부확률에 의해서 넘어가게된다.<br>
그 뒤 식.1에서 식.2로 넘어가는것은 표본공간 S가 A들의 합집합으로 이루어졌고 B가 표본공간S위에서 정의된 사건이기 때문에 가능하다.<br>
<br>
그리고 식의 마지막부분을 보게되면 사후확률이 되는데 사전확률과 우도로 이루어져있다.<br>
따라서 사후확률의 경우 사전확률과 우도의 조합으로 구할 수 있다.<br>
<br>
베이즈정리는 사후확률을 식.1로 구할 수 없을때 사전확률과 우도로 구할수 있다는 정리이다.<br>

#### 베이즈정리 예시 (Bayes' theorem example)
<hr>
크기와 모양이 같은 공이 상자 A에는 검은공2개, 흰공2개가 들어있고, 상자 B에는 검은공1개, 흰공2개가 들어있다고하자.<br>
이때 임의의 한 상자에서 공을 1개 꺼냇더니 검은공이었고 임의의 한 상자에 남은공이 흰공2개일 확률은?
<hr>
문제 해석<br>

- 구해야 하는값 : 두 상자 A, B중 임의로 한 상자를 골라서 검은공을 한개 꺼냈는데 남은공이 흰공 2개일 확률<br>
- 해석 : 검은공을 뽑았을 때, B상자일 확률 

문제 해석을 다 했으니 저 식에 대입해보자.

<img src="https://github.com/KimMinJoo/KimMinJoo.github.io/blob/master/images/BayesTheoremExample.jpg?raw=true"/>


### Naive Bayes란?
특성들 사이의 독립을 가정하는 베이즈 정리를 적용한 확률 분류기의 일종<br>
<br>
간단하게 얘기하자면 속성 x,y가 주어졌을때, <br>
p1(x,y) > p2(x,y)이면, 분류항목 1에 속하고,<br>
p1(x,y) < p2(x,y)이면, 분류항목 2에 속한다.<br>
(여기서 p1,p2함수는 속성x,y가 주어졌을 때 속성 x,y가 분류항목1, 2에 속할 확률을 반환하는 함수이다.)<br>

### Naive Bayes의 특징
- 장점 : 소량의 데이터를 가지고 작업이 이루어지며, 여러 개의 분류 항목을 다룰 수 있다.
- 단점 : 입력 데이터를 어떻게 준비하느냐에 따라 민감하게 작용한다.
- 적용 : 명목형 값. 
<br>
### Naive Bayes와 Bayes Theorem과의 관계
위의 설명에서 Naive Bayes에 대해서 간단하게 얘기한다면,
p1(x,y) > p2(x,y)이면, 분류항목 1에 속하고,<br>
p1(x,y) < p2(x,y)이면, 분류항목 2에 속한다.<br>
라고 얘기했다. 하지만 이러한 두가지 규칙으로는 전체 내용을 설명할 수 없다.<br>
<br>
사실 p1(x,y)는 p(c1|x,y)이며 p2(x,y)는 p(c2|x,y)이다.<br>
즉 풀어서 얘기하면 어떤조건 x,y가 들어왔을때 어떤 분류항목일지, 그 조건부 확률의 비교인것이다.<br>
식으로 표현하면 아래와 같다.<br>
<img src="https://github.com/KimMinJoo/KimMinJoo.github.io/blob/master/images/NaiveBayes%EA%B7%9C%EC%B9%99.jpg?raw=true"/>
<br>
결국 p(c1|x,y)과 p(c2|x,y)를 비교할 때, p(x,y|c1), p(c1), p(x,y|c2), p(c2)만 안다면 대소비교가 가능한것입니다.<br>
따라서 p(c|x,y)를 모르더라도 p(x,y|c)와 p(c)만 알 수 있는 상황에서 나이브베이즈를 사용하면 된다.<br>
<br>
한가지 예를 들어서 설명을 하겠다.
- c1,c2는 폭력적문장, 비폭력적문장이라는 조건이다.
- s는 s라는 어떤 문장이 들어오는 조건이다.

즉 따라서 P(c1|s)과 P(c2|s)를 비교할 때,<br>
풀어서 얘기하면 s라는 문장이 들어왔을 때 이 문장이 c1일 확률이 높냐 c2일 확률이 높냐를 구하기 위해서는 아래와같은 4가지만 알면된다.<br>
- P(s|c1) : 폭력적인 문장에 s라는 문장에 존재하는 단어들이 나올 확률.
- P(c1) : 폭력적인 문장일 확률.
- P(s|c2) : 비폭력적인 문장에 s라는 문장에 존재하는 단어들이 나올 확률.
- P(c2) : 비폭력적인 문장이 나올 확률.

저런 조건들은 여러 트레이닝데이터(폭력,비폭력이 구분된 문장들)을 가지고 구할 수 있다.

### Naive Bayes예제 코드