---
layout: post
title: kNN 알고리즘
---
# k-최근접 이웃 알고리즘(k-Nearest Neighbors)

## 1.동작방식<br>
트레이닝 데이터가 있으며 모든 데이터는 분류항목표시(labels)가 붙어 있다.<br>
따라서 각가의 데이터가 어떤 분류항목으로 구분되는지 알 수 있다.<br>
이후 새로운 데이터가 주어졌을 때 기존의 모든 데이터와 비교한다.<br>
그리고 가장 유사한(근접한) 데이터의 분류 항목 표시를 살펴본다.<br>
이때, 분류 항목을 이미 알고 있는 데이터 집합에서 상위 k개의 유사한 데이터를 살펴본다.<br>
그 뒤 k개의 가장 유사한 데이터들 중 다수결을 통해 새로운 데이터의 분류 항목을 결정한다.<br>

### - 장점
    - 높은 정확도
    - 오류데이터에 둔감
    - 데이터에 대한 가정이 없음
    
### - 단점
    - 계산비용이 높음
    - 많은 메모리 요구
    
### - 적용
    - 수치형
    - 명목형

### - 기본 개념
   - 훈련집합의 모든 데이터에 '분류항목'이 존재하며 훈련집합의 분류항목을 통해 새로운 데이터를 학습시키는 알고리즘.
   - 분류 항목이 없는 새로운 데이터의 학습법
        - 이미 분류항목을 알고있는 데이터 집합에서 상위 k개를 뽑아 다수결을 통해 결정한다.


## 2.sudo 코드<br>
<pre>
데이터 집합에 있는 모든 측정값 반복<br>
    입력값과 현재 데이터 사이의 거리 계산<br>
    오름차순으로 거리 정렬<br>
    가장 가까운 거리에 있는 데이터 k개 추출<br>
    k개 데이터에서 가장 많은 분류 찾기<br>
가장많은 분류 반환<br>
</pre>


## 3.코드
```python
def kNNAlgorithm(inX, dataSet, labels, k):
    #shape함수의 경우 array의 모양을 나타냄 위의 createDataSet에 따르면 4,2의 행렬이므로 [0]은 4, [1]은 2를 나타낸다.
    dataSetSize = dataSet.shape[0]
    #inX의 값으로 (dataSetSize,1)사이즈로 만들고 dataSet만큼 빼 dataSet과의 X축거리, Y축거리를 확보한다
    diffMat = tile(inX, (dataSetSize, 1)) - dataSet
    #거리를 구하기위해 제곱을 한다.
    sqDiffMat = diffMat ** 2
    print(sqDiffMat)
    #axis = 0은 행  1은 열을 의미함. 행별로 더하는게 아니라 열별로 더한다.
    sqDistances = sqDiffMat.sum(axis = 1)
    #루트를 씌워 최종 거리를 구한다.
    distances = sqDistances ** 0.5
    #거리별로 index들을 정렬한다.
    sortedDistIndicies = distances.argsort()
    classCount={}
    for i in range(k):
        #가장 가까운 인덱스들의 라벨을 꺼낸다.
        voteIlabel = labels[sortedDistIndicies[i]]
        #classCount dictionary에서 각 라벨의 횟수를 가져와 1을 올려준다. 다만 없을경우 0으로 가져온다.
        classCount[voteIlabel] = classCount.get(voteIlabel, 0) + 1
    #dictionary 정렬 itemgetter가 0이면 키기준, 1이면 value기준
    sortedClassCount = sorted(classCount.items(), key = operator.itemgetter(1), reverse = True)
    #정렬된 dictionary에서 가장 첫밸류 리턴.
    return sortedClassCount[0][0]
```


## 4.예제<br>
헬렌이라는 사람이 데이트사이트에서 추천인을 받아서 데이트를 했을 때<br>
좋아하지 않았던 사람들, 조금 좋아했던사람들, 많이 좋아했던 사람들이 있던것을 알고,<br>
앞으로 만날 사람들을 걸러내기위해 분류를 하기위해 만든 프로그램.
### - 데이터
    - 머신러닝 인 액션에서 제공된 데이터<br>
### - 데이터 형식 
    - 연간 항공 마일리지 수
    - 비디오 게임으로 보내는 시간의 비율
    - 주당 아이스크림 소비량(L)
    - 라벨
### - 라벨
    - 좋아하지 않았던 사람들(1)
    - 조금 좋아했던 사람들(2)
    - 많이 좋아했던 사람들(3)
### - 분석
    - 데이터를 시각적으로 보여주기 위해 matplotlib 사용<br>
### - 훈련
    - kNN알고리즘에는 적용되지 않음
### - 검사
    - 헬렌이 준 검사용 예제 데이터의 일부를 사용하기 위한 함수 작성
    - 검사용 예제는 검사에 사용되지 않는 예제에 반해 분류되어있다
    - 예측된 분류 항목이 실질적인 분류 항목과 일치하지 않으면 오류
### - 사용
    - 헬렌이 입력한 몇 가지 데이터를 토대로 누구를 좋아할지 예측하는 커넨드라인 프로그램

## 4.1. 예제 첫번째 단계 : 텍스트 파일의 데이터 구문 분석하기<br>
준비된 데이터는 텍스트 파일안에 있으며 datingTestSet.txt파일에 있다고 한다.<br>
이 데이터를 우리의 분류기에 사용하기 전에 분류기에 사용할 수 있는 데이터의 형태로 변경해야 한다.<br>
따라서 아래와 같은 함수를 만들어준다.
```python
'''
file을 matrix로 변환시켜주는 함수
filename 파일이름
k 파일 한 줄에 들어가있는 데이터의 수
'''
def file2matrix(filename, k):
    #파일을 읽는다.
    fr = open(filename)
    #파일의 각 줄로 배열을 만든다.
    arrayOfLines = fr.readlines()
    #파일의 전체 줄 수.
    numberOfLines = len(arrayOfLines)
    #0으로된 (파일줄수, k)의 matrix를 만든다.
    returnMat = zeros((numberOfLines,k))
    #라벨을 따로 저장할 라벨 배열.
    classLabelVector = []
    index = 0
    #각 라인의 데이터를 매트릭스와 라벨베열에 넣어준다.
    for line in arrayOfLines:
        #문자 양끝 공백제거. (필요에 따라 생략가능)
        line = line.strip()
        #라인을 tab기준으로 스플릿하여 리스트로 만든다.
        listFromLine = line.split('\t')
        #매트릭스의 index번째에 리스트를 넣어준다.
        returnMat[index,:] = listFromLine[0:k]
        #데이터 마지막에 있는 라벨을 라벨 리스트에 저장해준다.
        classLabelVector.append(int(listFromLine[-1]))
        index += 1
    return returnMat,classLabelVector
```

## 4.2. 예제 두번째 단계 : matplotlib로 scatter플롯 생성하기.(분석)<br>
matplotlib로부터 데이터의 scatter 플롯을 만들어 데이터를 더 세밀하게 관찰해 보자.<br>
아래의 코드로 확인할 수 있다.

```python
fig = plt.figure()
#figure의 구분값( 1 1 1 의 경우 1x1의 첫번째  212 의 경우 2x1의 두번째.....)
ax = fig.add_subplot(221)
ax.set(title='mileage and video game', xlabel='mileage', ylabel='video game')
#(y축, x축 크기, 색(각 점마다 색을 주기위해 배열)
#연간 항송 마일리지에 비례한 비디오 게임으로 보내는 시간의 비율
ax.scatter(datingDataMat[:, 0], datingDataMat[:, 1], 30.0*array(datingLabels), array(datingLabels))
ax2 = fig.add_subplot(222)
ax2.set(title='video game and ice cream', xlabel='video game', ylabel='ice cream')
ax2.scatter(datingDataMat[:, 1], datingDataMat[:, 2], 30.0*array(datingLabels), array(datingLabels))
ax3 = fig.add_subplot(223)
ax3.set(title='mileage and ice cream', xlabel='mileage', ylabel='ice cream')
ax3.scatter(datingDataMat[:, 0], datingDataMat[:, 2], 30.0*array(datingLabels), array(datingLabels))
plt.show()
```

## 4.3 예제 세번째 단계 : 수치형 값 정규화하기<br>
정규화가 필요한 이유는 각 데이터들의 단위가 다르기 때문이다.<br>
예를들어 아래와 같은 두 데이터가 있다고 보자.<br>
|mileage|video game|ice cream|<br>
|40920	|8.326976  |0.953952 |<br>
14488	|7.153469  |1.673904 |<br>
이경우 mileage가 다른 두 항목에 비해 단위가 특출나게 크기때문에, mileage 값에 따라 두 데이터의 거리가 정해질 것이다.<br>
하지만 단위가 크다고 가장 중요한 항목은 아닐 수 있다.<br>
따라서 서로 다른 범위에 놓인 데이터들을 분석할 때 대부분의 경우 정규화가 필요하다.<br>
일반적인 정규화는 0 ~ 1, -1 ~ 1의 범위를 정해 두고 데이터들을 저 안의 알맞은 수로 변환한다.<br>
아래는 0 ~ 1사이의 범이로 정규화를 시키는 정규화 공식이다.<br>
newValue = (oldValue - min) / (max - min)<br>
정규화 단계를 분류기에 적용시키는게 복잡해 보일 수 있지만 이 단계는 좋은 결과를 얻도록 하는데 가치가 있다.<br>
<br>
아래의 코드는 0 ~ 1 사이로 데이터를 정규화 시키는 코드이다.<br>
```python
def autoNormalizationFunc(dataSet):
    #min, max와 max-min을 구한다.
    #min, max의 경우 각 열에서 min, max를 구한 1행짜리 list가 나온다.
    minValue = dataSet.min(0)
    maxValue = dataSet.max(0)
    range = maxValue - minValue

    m = dataSet.shape[0]
    #tile을 이용하여 먼저 빼주고 나눠준다.
    resultDataSet = dataSet - tile(minValue, (m, 1))
    resultDataSet = resultDataSet / tile(range, (m,1))
    return resultDataSet
```

## 4.4 예제 네번째 단계 : 분류기 검사하기 (검증)
정규화까지 마치고나면 하나의 형식으로 이루어진 데이터를 가지게 된 것이다.<br>
따라서 이를 사용할 수 있게 되었으므로 분류기를 검사 할 수 있다.<br>
기계 학습에서 한가지 공통된 작업이 있는데 그것이 이 단계이다.<br>
성능을 검증하는 방법중 한가지 방법은 현재 가지고있는 데이터의 일부분, 즉 90%를 가지고 학습에 사용을 하고 나머지 10%의 데이터로 분류기를 검사하는 방법이다.
검사에 사용될 10%데이터는 무작위로 선택된다.<br><br>
분류기의 성능은 오류율을 가지고 측정할 수 있다.<br>
분류에서 오류율은 아래와 같다.<br>
오류율 = 잘못 분류된 데이터의 개수 / 전체 데이터의 개수<br>

아래는 헬렌의 데이트 데이타 분류기 검사 코드이다.
```python
def datingClassTest():
    #비율
    hoRatio = 0.10
    
    #데이터 읽기
    datingDataMat ,datingLabels = file2matrix('datingTestSet2.txt', 3)
    
    #데이터 정규화
    datingDataMat = autoNormalizationFunc(datingDataMat)
    
    #총 갯수 구하기
    m= datingDataMat.shape[0]
    numTestVecs = int(m * hoRatio)
    
    #에러 카운트 초기화
    errorCount = 0.0
    
    #검증 시작.
    for i in range(numTestVecs):
     classifierResult = bookkNNAlgorithm.kNNAlgorithm(datingDataMat[i, :], datingDataMat[numTestVecs:m, :], datingLabels[numTestVecs:m], 3)
     
     #오류 상황
     if (classifierResult != datingLabels[i]):
         print("the classifier came back with : %d, the real answer is: %d"%(classifierResult, datingLabels[i]))
         errorCount += 1.0
            
    #결과 출력
    print("the total error rate is %f"% (errorCount/float(numTestVecs)))
```
